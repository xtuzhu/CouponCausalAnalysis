# 强化学习篇

[TOC]

1. ### 多臂老虎机

   1. 定义：**multi-armed bandit，MAB**，多臂老虎机不存在状态信息，只有动作和奖励

      <img src="C:\Users\13560\AppData\Roaming\Typora\typora-user-images\image-20230610233636387.png" alt="image-20230610233636387" style="zoom:50%;" />

   2. 过程：每台机子拉杆都对应一个奖励的概率分布记为$P$（未知）， 拉动一次获得奖励$r$

   3. 目标：$T$次操作后获得最大的奖励

   4. 形式化表达

      多臂老虎机问题可以表示为一个元组$<A,R>$，其中：

      -  $A$为动作集合，其中一个动作表示拉动一个拉杆。若多臂老虎机一共有$K$根拉杆，那动作空间就是集合$\left\{a_1, \ldots, a_K\right\}$，我们用$a_t \in A$表示任意一个动作；
      -  $R$为奖励概率分布，拉动每一根拉杆的动作$a$都对应一个奖励概率分布$R(r\mid a)$，不同拉杆的奖励分布通常是不同的。

      假设每个时间步只能拉动一个拉杆，多臂老虎机的目标为最大化一段时间$T$步内累积的奖励: $\max \sum_{t=1}^T r_t, r_t \sim \mathcal{R}\left(\cdot \mid a_t\right)$。其中$a_t$表示在第$t$时间步拉动某一拉杆的动作，$r_t$表示动作$a_t$获得的奖励。

2. ### 马尔可夫决策过程

   1. 定义：当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有**马尔可夫性质**（Markov property）。**马尔可夫过程**（Markov process）指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**（Markov chain）

   2. 形式化表达

      马尔可夫决策过程由元组构成$<S,A,P,r,\gamma>$，其中：

      - $S$是状态的集合；
      - $A$是动作的集合；
      - $\gamma$是折扣因子；
      - $r(s,a)$是奖励函数，此时奖励可以同时取决于状态$s$和动作$a$
      - $P(s' \mid s,a)$是状态转移函数，表示在状态$s$执行动作$a$之后到达状态$s'$的概率。

   3. 策略输出

      智能体的策略（Policy）通常用字母$\pi$表示。策略$\pi(a \mid s) = P(A_t=a \mid S_t=s)$是一个函数，表示在输入状态$s$情况下采取动作$a$的概率。当一个策略是**确定性策略**（deterministic policy）时，它在每个状态时只输出一个确定性的动作，即只有该动作的概率为 1，其他动作的概率为 0；当一个策略是**随机性策略**（stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。




根据是否具有环境模型，强化学习算法分为两种：**基于模型的强化学习**（model-based reinforcement learning）和**无模型的强化学习**（model-free reinforcement learning）。无模型的强化学习根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计。

动态规划算法（策略迭代和价值迭代），则是基于模型的强化学习方法，在这两种算法中环境模型是事先已知的

时序差分算法（Sarsa 和 Q-learning 算法），便是两种无模型的强化学习方法

1. ### 动态规划算法

   基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。但是，**现实中的白盒环境很少，这也是动态规划算法的局限之处，我们无法将其运用到很多实际场景中**。另外，策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的。

2. ### 时序差分算法

   不同于动态规划算法，无模型的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是直接使用和环境交互的过程中采样到的数据来学习，这使得它可以被应用到一些简单的实际场景中。

   其中经典的是Q-Learning算法

3. ### Q-Learning

   <img src="C:\Users\13560\AppData\Roaming\Typora\typora-user-images\image-20230611004243885.png" alt="image-20230611004243885" style="zoom:67%;" />

   需要强调的是，Q-learning 的更新并非必须使用当前贪心策略$arg max_aQ(s,a)$采样得到的数据，因为给定任意$(s,a,r,s')$都可以直接根据更新公式来更新$Q$，称为离线策略



### 230613 讨论

1. 看相关书籍，更改形式化表达，动态规划+时序差分算法https://rl.qiwihui.com/zh_CN/latest/preface2nd.html
2. 找代码：用贪心算法解决多臂老虎机问题
3. 找相关问题和数据：建议社会学科相关
4. 阿里离散价格方法，详看
5. 





1. ### DQN

   1. 引言：Q-learning 算法中，以矩阵的方式建立了一张存储每个状态下所有动作$Q$值的表格。表格中的每一个动作价值$Q(s,a)$表示在状态$s$下选择动作$a$然后继续遵循某一策略预期能够得到的期望回报。然而，这种用表格存储动作价值的做法只在环境的状态和动作都是离散的，并且空间都比较小的情况下适用。**当状态或者动作数量非常大的时候，这种做法就不适用了**。例如，当状态是一张 RGB 图像时，假设图像大小是$210 \times 160 \times 3$，此时一共有$256^{(210 \times 160 \times 3)}$种状态，在计算机中存储这个数量级的$Q$值表格是不现实的。更甚者，当状态或者动作连续的时候，就有无限个状态动作对，我们更加无法使用这种表格形式来记录各个状态动作对的$Q$值。

      对于这种情况，我们需要用函数拟合的方法来估计$Q$值，即将这个复杂的$Q$值表格视作数据，使用一个参数化的函数$Q_\theta$来拟合这些数据。很显然，这种函数拟合的方法存在一定的精度损失，因此被称为近似方法。 DQN 算法便可以用来解决连续状态下离散动作的问题。

   2. 例子：在车杆环境中，有一辆小车，智能体的任务是通过左右移动保持车上的杆竖直，若杆的倾斜度数过大，或者车子离初始位置左右的偏离程度过大，或者坚持时间到达 200 帧，则游戏结束。

      ![img](https://hrl.boyuai.com/static/cartpole.e4a03ca5.gif)

      现在我们想在类似车杆的环境中得到动作价值函数$Q(s,a)$，由于状态每一维度的值都是连续的，无法使用表格记录，因此一个常见的解决方法便是使用函数拟合，我们将用于拟合函数函数的神经网络称为**Q 网络**

      <img src="C:\Users\13560\AppData\Roaming\Typora\typora-user-images\image-20230611005736604.png" alt="image-20230611005736604" style="zoom:50%;" />

      2.1 经验回放

      在原来的 Q-learning 算法中，每一个数据只会用来更新一次值。为了更好地将 Q-learning 和深度神经网络结合，DQN 算法采用了**经验回放**（experience replay）方法，具体做法为维护一个**回放缓冲区**，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练。这么做可以起到以下两个作用。

      （1）**使样本满足独立假设**。在 MDP 中交互采样得到的数据本身不满足独立假设，因为这一时刻的状态和上一时刻的状态有关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。

      （2）**提高样本效率**。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习。

   3. 代码实现

      采用一层 128 个神经元的全连接并以 ReLU 作为激活函数。可以看到，DQN 的性能在 100 个序列后很快得到提升，最终收敛到策略的最优回报值 200

      <img src="C:\Users\13560\AppData\Roaming\Typora\typora-user-images\image-20230611194521842.png" alt="image-20230611194521842" style="zoom:67%;" />







【参考资料】

1. [DQN 算法 (boyuai.com)](https://hrl.boyuai.com/chapter/2/dqn算法)